# Data Processing Workflow Example
# This workflow demonstrates intermediate Lacquer features including:
# - Multiple specialized agents
# - Conditional execution based on outputs
# - State management for tracking progress
# - Structured output parsing

version: "1.0"
metadata:
  name: data-processor
  description: |
    Processes and analyzes data with validation, transformation,
    and quality checks. Shows best practices for multi-step workflows.
  author: examples@lacquer.ai
  tags:
    - data-processing
    - validation
    - analytics
  version: 1.0.0

# Define specialized agents for different tasks
agents:
  # Validator agent with low temperature for consistent results
  validator:
    provider: openai
    model: gpt-4
    temperature: 0.1
    system_prompt: |
      You are a data validation expert. You:
      - Check data structure and format
      - Identify missing or invalid values
      - Ensure data meets quality standards
      - Return structured validation results
  
  # Analyzer agent for data insights
  analyzer:
    provider: openai
    model: gpt-4
    temperature: 0.3
    system_prompt: |
      You are a data analyst who:
      - Identifies patterns and trends
      - Calculates key statistics
      - Provides actionable insights
      - Returns analysis in JSON format
  
  # Transformer agent for data manipulation
  transformer:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.2
    system_prompt: |
      You are a data transformation specialist who:
      - Cleans and normalizes data
      - Applies requested transformations
      - Maintains data integrity
      - Documents all changes made

# Define workflow inputs
inputs:
  data:
    type: string
    description: The raw data to process (CSV, JSON, or plain text)
    required: true
  
  format:
    type: string
    description: Expected data format
    default: "auto"
  
  validation_rules:
    type: object
    description: Custom validation rules to apply
    default:
      require_headers: true
      allow_nulls: false
      min_rows: 1
  
  output_format:
    type: string
    description: Desired output format (json, csv, report)
    default: "json"

# Main workflow logic
workflow:
  # Initialize state for tracking progress
  state:
    processing_status: "started"
    validation_passed: false
    records_processed: 0
    errors: []
    warnings: []
  
  steps:
    # Step 1: Validate the input data
    - id: validate_data
      agent: validator
      prompt: |
        Validate this data according to the rules:
        
        Data: ${{ inputs.data }}
        Format: ${{ inputs.format }}
        Rules: ${{ toJSON(inputs.validation_rules) }}
        
        Return a JSON response with:
        - is_valid: boolean
        - format_detected: string
        - row_count: integer
        - column_count: integer (if applicable)
        - errors: array of error messages
        - warnings: array of warning messages
      outputs:
        is_valid:
          type: boolean
        format_detected:
          type: string
        row_count:
          type: integer
        column_count:
          type: integer
        errors:
          type: array
        warnings:
          type: array
      updates:
        validation_passed: ${{ steps.validate_data.outputs.is_valid }}
        errors: ${{ steps.validate_data.outputs.errors }}
        warnings: ${{ steps.validate_data.outputs.warnings }}
    
    # Step 2: Stop if validation failed
    - id: report_validation_failure
      condition: ${{ !steps.validate_data.outputs.is_valid }}
      agent: validator
      prompt: |
        Generate a validation failure report:
        - Errors found: ${{ join(steps.validate_data.outputs.errors, '; ') }}
        - Warnings: ${{ join(steps.validate_data.outputs.warnings, '; ') }}
        - Detected format: ${{ steps.validate_data.outputs.format_detected }}
        
        Provide recommendations for fixing the issues.
      updates:
        processing_status: "failed_validation"
    
    # Step 3: Analyze valid data
    - id: analyze_data
      condition: ${{ steps.validate_data.outputs.is_valid }}
      agent: analyzer
      prompt: |
        Analyze this validated data:
        
        Data: ${{ inputs.data }}
        Format: ${{ steps.validate_data.outputs.format_detected }}
        Rows: ${{ steps.validate_data.outputs.row_count }}
        Columns: ${{ steps.validate_data.outputs.column_count }}
        
        Provide analysis including:
        - summary_statistics: key metrics
        - patterns_found: identified patterns
        - anomalies: unusual data points
        - quality_score: 0-100
        - insights: actionable findings
      outputs:
        summary_statistics:
          type: object
        patterns_found:
          type: array
        anomalies:
          type: array
        quality_score:
          type: integer
        insights:
          type: array
    
    # Step 4: Transform data if quality is acceptable
    - id: transform_data
      condition: ${{ steps.analyze_data.outputs.quality_score >= 70 }}
      agent: transformer
      prompt: |
        Transform the data for output format: ${{ inputs.output_format }}
        
        Original data: ${{ inputs.data }}
        Analysis insights: ${{ join(steps.analyze_data.outputs.insights, '; ') }}
        
        Apply these transformations:
        1. Clean any identified anomalies
        2. Normalize formats
        3. Structure for ${{ inputs.output_format }} output
        
        Return the transformed data and a list of changes made.
      outputs:
        transformed_data:
          type: string
        changes_made:
          type: array
      updates:
        records_processed: ${{ steps.validate_data.outputs.row_count }}
        processing_status: "completed"
    
    # Step 5: Handle low quality data
    - id: handle_low_quality
      condition: ${{ steps.validate_data.outputs.is_valid && steps.analyze_data.outputs.quality_score < 70 }}
      agent: analyzer
      prompt: |
        The data quality score is low: ${{ steps.analyze_data.outputs.quality_score }}/100
        
        Issues found:
        - Anomalies: ${{ length(steps.analyze_data.outputs.anomalies) }}
        - Warnings: ${{ length(state.warnings) }}
        
        Recommend whether to:
        1. Proceed with warnings
        2. Request manual review
        3. Attempt automatic fixes
      outputs:
        recommendation:
          type: string
      updates:
        processing_status: "low_quality_warning"
  
  # Define workflow outputs
  outputs:
    # Status information
    status: ${{ state.processing_status }}
    success: ${{ state.processing_status == 'completed' }}
    
    # Validation results
    validation:
      passed: ${{ state.validation_passed }}
      format: ${{ steps.validate_data.outputs.format_detected }}
      errors: ${{ state.errors }}
      warnings: ${{ state.warnings }}
    
    # Processing results (conditional)
    results: ${{ state.processing_status == 'completed' ? {
      'transformed_data': steps.transform_data.outputs.transformed_data,
      'records_processed': state.records_processed,
      'quality_score': steps.analyze_data.outputs.quality_score,
      'insights': steps.analyze_data.outputs.insights,
      'changes': steps.transform_data.outputs.changes_made
    } : null }}
    
    # Analysis (if performed)
    analysis: ${{ steps.analyze_data.outputs || null }}
